{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSUNAMI Package Vignette\n",
    "\n",
    "This notebook demonstrates the key features of the TSUNAMI package, a comprehensive framework for survival analysis and multi-task learning.\n",
    "\n",
    "In this vignette, we will:\n",
    "1. Load and preprocess the EBMT dataset (European Bone Marrow Transplantation registry data)\n",
    "2. Create synthetic target variables to demonstrate multi-task capability\n",
    "3. Implement three demonstrations:\n",
    "   - Single Risk Survival Analysis\n",
    "   - Competing Risks Analysis\n",
    "   - Multi-Task Learning\n",
    "4. Visualize results with various plots:\n",
    "   - Survival curves with uncertainty quantification\n",
    "   - Feature importance using different methods\n",
    "   - Feature effect visualization (partial dependence, ICE curves, interactions)\n",
    "   - Task-specific outputs\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Import TSUNAMI modules\n",
    "from enhanced_deephit.data.processing import DataProcessor\n",
    "from enhanced_deephit.models import EnhancedDeepHit\n",
    "from enhanced_deephit.models.tasks.base import TaskHead\n",
    "from enhanced_deephit.models.tasks.standard import ClassificationHead, RegressionHead\n",
    "from enhanced_deephit.models.tasks.survival import SingleRiskHead, CompetingRisksHead\n",
    "from enhanced_deephit.visualization.importance.importance import (\n",
    "    PermutationImportance,\n",
    "    ShapImportance,\n",
    "    IntegratedGradients,\n",
    "    AttentionImportance\n",
    ")\n",
    "from enhanced_deephit.visualization.survival_plots import (\n",
    "    plot_survival_curve,\n",
    "    plot_cumulative_incidence,\n",
    "    plot_calibration_curve\n",
    ")\n",
    "from enhanced_deephit.visualization.feature_effects import (\n",
    "    plot_partial_dependence,\n",
    "    plot_ice_curves,\n",
    "    plot_feature_interaction\n",
    ")\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create output directory for plots\n",
    "os.makedirs(\"plots\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Exploring the EBMT Dataset\n",
    "\n",
    "We'll start by loading the European Bone Marrow Transplantation (EBMT) registry dataset, which contains information about patients who received bone marrow transplants and their outcomes. \n",
    "\n",
    "This dataset includes various patient characteristics, treatment details, and survival outcomes, making it ideal for demonstrating survival analysis methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the EBMT dataset\n",
    "print(\"Loading EBMT dataset...\")\n",
    "ebmt_data = pd.read_csv('ebmt3.csv')\n",
    "print(f\"Dataset loaded: {ebmt_data.shape[0]} patients, {ebmt_data.shape[1]} variables\")\n",
    "\n",
    "# Display dataset information\n",
    "print(\"\\nDataset Information:\")\n",
    "ebmt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable types and missing values\n",
    "print(\"Variable types and counts:\")\n",
    "print(ebmt_data.dtypes)\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(ebmt_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the time-to-event variables\n",
    "print(\"Time-to-event statistics:\")\n",
    "print(\"\\ntime:\")\n",
    "print(ebmt_data['time'].describe())\n",
    "\n",
    "# Event counts\n",
    "print(\"\\nEvent counts:\")\n",
    "print(\"\\nstatus:\")\n",
    "print(ebmt_data['status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variable distributions\n",
    "print(\"Categorical variable distributions:\")\n",
    "for cat_var in ['dissub', 'age', 'match', 'tcd']:\n",
    "    print(f\"\\n{cat_var}:\")\n",
    "    print(ebmt_data[cat_var].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Synthetic Variables for Multi-Task Demonstration\n",
    "\n",
    "To demonstrate the multi-task learning capabilities of TSUNAMI, we'll create additional synthetic target variables:\n",
    "1. A binary classification target (`binary_outcome`)\n",
    "2. A regression target (`biomarker`)\n",
    "3. A competing risks cause indicator (`cr_cause`)\n",
    "\n",
    "We'll also rename some columns for clarity in modeling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Rename columns for clarity in modeling\nebmt_data = ebmt_data.rename(columns={\n    'time': 'survival_time',\n    'status': 'event_indicator',\n    'match': 'drmatch'  # More descriptive name\n})\n\n# Simplify event indicator for demonstration (0: censored, 1: event)\n# Original dataset has 0, 1, 2, 3, 4, 5, 6 statuses\nebmt_data['event_indicator'] = (ebmt_data['event_indicator'] > 0).astype(int)\n\n# Synthetic binary classification target\nebmt_data['binary_outcome'] = np.random.binomial(1, 0.3, size=len(ebmt_data))\n\n# Synthetic regression target (e.g., biomarker value) - not using survival_time to avoid data leakage\nage_effect = 0.3 * (ebmt_data['age'] == \">40\").astype(int) * 10\ndissub_effect = 0.2 * (ebmt_data['dissub'] == \"ALL\").astype(int) * 5\nebmt_data['biomarker'] = age_effect + dissub_effect + np.random.normal(15, 5, size=len(ebmt_data))\n\n# Synthetic competing risks (create a cause indicator from 'cod' column)\n# 0: censored, 1: Relapse, 2: Death without relapse\nebmt_data['cr_cause'] = 0  # Default to censored\nebmt_data.loc[ebmt_data['event_indicator'] == 1, 'cr_cause'] = 1  # Relapse as default event\n# Randomly assign half of the events to cause 2 (death) for demonstration\nevent_mask = ebmt_data['event_indicator'] == 1\nevent_indices = ebmt_data[event_mask].index\ncause2_indices = np.random.choice(event_indices, size=len(event_indices) // 2, replace=False)\nebmt_data.loc[cause2_indices, 'cr_cause'] = 2\n\nprint(\"Synthetic variables created:\")\nprint(\" - 'binary_outcome': Binary classification target\")\nprint(\" - 'biomarker': Regression target\")\nprint(\" - 'cr_cause': Competing risks cause (0: censored, 1: relapse, 2: death)\")\n\n# View the updated dataframe\nebmt_data.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Now we'll preprocess the data using TSUNAMI's `DataProcessor` class, which handles:\n",
    "- Missing value imputation\n",
    "- Feature normalization\n",
    "- Categorical variable encoding\n",
    "\n",
    "We'll then prepare the data for our survival models by discretizing time into bins and formatting the targets appropriately."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Define feature columns\nnumeric_features = []  # Not using survival_time as a feature to avoid data leakage\ncategorical_features = ['dissub', 'age', 'drmatch', 'tcd']\n\n# Create a preprocessor\npreprocessor = DataProcessor(\n    num_impute_strategy='median',\n    cat_impute_strategy='most_frequent',\n    normalize='robust'\n)\n\n# Prepare feature dataframe - only using actual features, not target variables\nfeature_df = ebmt_data[categorical_features].copy()  # No numeric features used\n\n# Fit the preprocessor and transform the data\npreprocessor.fit(feature_df)\nprocessed_df = preprocessor.transform(feature_df)\n\nprint(\"Preprocessed data:\")\nprocessed_df.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(processed_df.values, dtype=torch.float32)\n",
    "\n",
    "# Prepare time and event data for SingleRiskHead\n",
    "# Discretize time into bins\n",
    "num_time_bins = 20\n",
    "max_time = np.percentile(ebmt_data['survival_time'], 99)\n",
    "bin_edges = np.linspace(0, max_time, num_time_bins + 1)\n",
    "time_bins = np.digitize(ebmt_data['survival_time'], bin_edges) - 1\n",
    "time_bins = np.clip(time_bins, 0, num_time_bins - 1)\n",
    "\n",
    "# Create target format for SingleRiskHead\n",
    "# [event_indicator, time_bin, one_hot_encoding]\n",
    "single_risk_target = np.zeros((len(ebmt_data), 2 + num_time_bins))\n",
    "single_risk_target[:, 0] = ebmt_data['event_indicator'].values\n",
    "single_risk_target[:, 1] = time_bins\n",
    "\n",
    "# One-hot encoding of time\n",
    "for i in range(len(ebmt_data)):\n",
    "    if ebmt_data['event_indicator'].iloc[i]:\n",
    "        # For events, mark the event time\n",
    "        single_risk_target[i, 2 + int(time_bins[i])] = 1\n",
    "    else:\n",
    "        # For censored, mark all times after censoring as unknown (-1)\n",
    "        single_risk_target[i, 2 + int(time_bins[i]):] = -1\n",
    "\n",
    "# Create target for CompetingRisksHead\n",
    "# [event_indicator, time_bin, cause_index, one_hot_encoding]\n",
    "competing_risks_target = np.zeros((len(ebmt_data), 3 + num_time_bins * 2))\n",
    "competing_risks_target[:, 0] = (ebmt_data['cr_cause'] > 0).astype(float)  # Event indicator\n",
    "competing_risks_target[:, 1] = time_bins\n",
    "competing_risks_target[:, 2] = ebmt_data['cr_cause'] - 1  # -1 for censored, 0 for cause 1, 1 for cause 2\n",
    "competing_risks_target[competing_risks_target[:, 2] < 0, 2] = -1  # Set censored to -1\n",
    "\n",
    "# Convert targets to tensors\n",
    "single_risk_tensor = torch.tensor(single_risk_target, dtype=torch.float32)\n",
    "competing_risks_tensor = torch.tensor(competing_risks_target, dtype=torch.float32)\n",
    "binary_tensor = torch.tensor(ebmt_data['binary_outcome'].values, dtype=torch.float32).unsqueeze(1)\n",
    "regression_tensor = torch.tensor(ebmt_data['biomarker'].values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating PyTorch Datasets and DataLoaders\n",
    "\n",
    "To efficiently feed data to our models, we'll create a custom PyTorch `Dataset` class that handles the various target formats required by different task heads. We'll then split the data into training, validation, and testing sets, and create DataLoaders for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader with proper formatting\n",
    "class SurvivalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, sr_targets=None, cr_targets=None, binary_targets=None, regression_targets=None):\n",
    "        self.X = X\n",
    "        self.sr_targets = sr_targets\n",
    "        self.cr_targets = cr_targets\n",
    "        self.binary_targets = binary_targets\n",
    "        self.regression_targets = regression_targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {'continuous': self.X[idx]}\n",
    "        targets = {}\n",
    "        \n",
    "        if self.sr_targets is not None:\n",
    "            targets['survival'] = self.sr_targets[idx]\n",
    "        \n",
    "        if self.cr_targets is not None:\n",
    "            targets['competing_risks'] = self.cr_targets[idx]\n",
    "            \n",
    "        if self.binary_targets is not None:\n",
    "            targets['binary'] = self.binary_targets[idx]\n",
    "            \n",
    "        if self.regression_targets is not None:\n",
    "            targets['regression'] = self.regression_targets[idx]\n",
    "            \n",
    "        item['targets'] = targets\n",
    "        return item\n",
    "\n",
    "# Create full multi-task dataset\n",
    "full_dataset = SurvivalDataset(\n",
    "    X_tensor, \n",
    "    sr_targets=single_risk_tensor,\n",
    "    cr_targets=competing_risks_tensor,\n",
    "    binary_targets=binary_tensor,\n",
    "    regression_targets=regression_tensor\n",
    ")\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "# Create random split\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Datasets prepared: {len(train_dataset)} train samples, {len(val_dataset)} validation samples, {len(test_dataset)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demonstration 1: Single Risk Survival Analysis\n",
    "\n",
    "Our first demonstration will focus on standard survival analysis with a single risk (event type). We'll use TSUNAMI's `SingleRiskHead` and `EnhancedDeepHit` model to predict survival probabilities over time.\n",
    "\n",
    "This example will showcase:\n",
    "- Building and training a deep survival model\n",
    "- Predicting survival curves\n",
    "- Quantifying prediction uncertainty\n",
    "- Calculating and visualizing feature importance\n",
    "- Analyzing feature effects with partial dependence plots and ICE curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create single risk model\n",
    "sr_task_head = SingleRiskHead(\n",
    "    name='survival',\n",
    "    input_dim=64,\n",
    "    num_time_bins=num_time_bins,\n",
    "    alpha_rank=0.1,\n",
    "    alpha_calibration=0.0  # Disable calibration loss due to issues\n",
    ")\n",
    "\n",
    "sr_model = EnhancedDeepHit(\n",
    "    num_continuous=X_tensor.shape[1],\n",
    "    targets=[sr_task_head],\n",
    "    encoder_dim=64,\n",
    "    encoder_depth=2,\n",
    "    encoder_heads=4,\n",
    "    include_variational=True,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Train the single risk model\n",
    "print(\"Training single risk model...\")\n",
    "sr_model.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=3,  # Reduced for demonstration\n",
    "    learning_rate=0.001,\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "print(\"Generating predictions for test set...\")\n",
    "X_test = torch.cat([test_dataset[i]['continuous'].unsqueeze(0) for i in range(len(test_dataset))])\n",
    "single_risk_preds = sr_model.predict(X_test)\n",
    "\n",
    "# Extract survival curves and uncertainty\n",
    "sr_survival_curves = single_risk_preds['task_outputs']['survival']['survival'].detach().numpy()\n",
    "sr_uncertainty = sr_model.compute_uncertainty(X_test[:10], num_samples=5)\n",
    "sr_survival_std = sr_uncertainty['survival']['std'].detach().numpy()\n",
    "\n",
    "# Verify survival curves start at 1.0\n",
    "print(\"Verifying survival curves start at 1.0:\")\n",
    "print(f\"First survival curve values: {sr_survival_curves[0, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualizing Survival Curves\n",
    "\n",
    "We'll visualize the predicted survival curves for multiple patients, as well as demonstrate the model's uncertainty quantification capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot survival curves\n",
    "time_points = bin_edges[:-1]  # Use bin start points as time points\n",
    "\n",
    "# Plot survival curves for first 5 patients\n",
    "fig1 = plot_survival_curve(\n",
    "    sr_survival_curves[:5], \n",
    "    time_points=time_points,\n",
    "    labels=[f'Patient {i+1}' for i in range(5)],\n",
    "    title=\"Survival Curves for Multiple Patients\"\n",
    ")\n",
    "plt.savefig(\"plots/sr_survival_curves.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with uncertainty\n",
    "fig2 = plot_survival_curve(\n",
    "    sr_survival_curves[0],\n",
    "    time_points=time_points,\n",
    "    uncertainty=sr_survival_std[0],\n",
    "    title=\"Survival Curve with Uncertainty\"\n",
    ")\n",
    "plt.savefig(\"plots/sr_survival_uncertainty.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Importance Analysis\n",
    "\n",
    "We'll use permutation-based feature importance to understand which features have the most influence on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for single risk model\n",
    "print(\"Calculating feature importance...\")\n",
    "perm_imp = PermutationImportance(sr_model)\n",
    "survival_targets = torch.cat([test_dataset[i]['targets']['survival'].unsqueeze(0) for i in range(100)])\n",
    "perm_importances = perm_imp.compute_importance(\n",
    "    {'continuous': X_test[:100]},\n",
    "    y=survival_targets,\n",
    "    n_repeats=2,\n",
    "    feature_names=processed_df.columns.tolist()\n",
    ")\n",
    "\n",
    "# Plot permutation importance\n",
    "fig3 = perm_imp.plot_importance(perm_importances)\n",
    "plt.title(\"Permutation Importance (Single Risk Model)\")\n",
    "plt.savefig(\"plots/sr_permutation_importance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature Effect Visualization\n",
    "\n",
    "Let's explore how specific features affect model predictions using partial dependence plots, ICE curves, and feature interaction plots."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Partial dependence plots\nprint(\"Generating partial dependence plots...\")\n# For a categorical feature embedding since we removed numeric features\ncategorical_idx = processed_df.columns.get_loc('dissub_embed_0')  \ncategorical_feature_name = processed_df.columns[categorical_idx]\n\nfig4 = plot_partial_dependence(\n    sr_model,\n    X_test[:100],\n    feature_idx=categorical_idx,\n    feature_name=categorical_feature_name,\n    target='risk_score',\n    title=f\"Partial Dependence of Risk Score on {categorical_feature_name}\"\n)\nplt.savefig(\"plots/sr_pd_categorical1.png\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# For another categorical feature\ncategorical_idx_other = processed_df.columns.get_loc('drmatch_embed_0')  \ncategorical_feature_name_other = processed_df.columns[categorical_idx_other]\n\nfig5 = plot_partial_dependence(\n    sr_model,\n    X_test[:100],\n    feature_idx=categorical_idx_other,\n    feature_name=categorical_feature_name_other,\n    target='risk_score',\n    title=f\"Partial Dependence of Risk Score on {categorical_feature_name_other}\"\n)\nplt.savefig(\"plots/sr_pd_categorical.png\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ICE curves for a categorical feature (using a different categorical feature for variety)\ncategorical_idx2 = processed_df.columns.get_loc('age_embed_0')  # Using a different categorical feature embedding\ncategorical_feature_name2 = processed_df.columns[categorical_idx2]\n\nfig6 = plot_ice_curves(\n    sr_model,\n    X_test[:20],\n    feature_idx=categorical_idx2,\n    feature_name=categorical_feature_name2,\n    target='risk_score',\n    title=f\"ICE Curves for {categorical_feature_name2}\"\n)\nplt.savefig(\"plots/sr_ice_curves.png\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Feature interaction between two categorical features\nfig7 = plot_feature_interaction(\n    sr_model,\n    X_test[:100],\n    feature1_idx=categorical_idx,\n    feature2_idx=categorical_idx2,\n    feature1_name=categorical_feature_name,\n    feature2_name=categorical_feature_name2,\n    target='risk_score',\n    title=f\"Interaction between {categorical_feature_name} and {categorical_feature_name2}\"\n)\nplt.savefig(\"plots/sr_feature_interaction.png\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demonstration 2: Competing Risks Analysis\n",
    "\n",
    "Next, we'll demonstrate competing risks analysis, which models multiple possible event types simultaneously. This is important in medical applications where patients might experience different outcomes (e.g., death from different causes).\n",
    "\n",
    "Our example will model two competing risks:\n",
    "1. Relapse (cause 1)\n",
    "2. Death without relapse (cause 2)\n",
    "\n",
    "We'll show how to:\n",
    "- Configure and train a competing risks model\n",
    "- Predict cause-specific cumulative incidence functions\n",
    "- Visualize overall survival in a competing risks setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create competing risks model\n",
    "cr_task_head = CompetingRisksHead(\n",
    "    name='competing_risks',\n",
    "    input_dim=64,\n",
    "    num_time_bins=num_time_bins,\n",
    "    num_risks=2,  # Two competing risks: relapse (1) and death (2)\n",
    "    alpha_rank=0.1,\n",
    "    alpha_calibration=0.0  # Disable calibration loss\n",
    ")\n",
    "\n",
    "cr_model = EnhancedDeepHit(\n",
    "    num_continuous=X_tensor.shape[1],\n",
    "    targets=[cr_task_head],\n",
    "    encoder_dim=64,\n",
    "    encoder_depth=2,\n",
    "    encoder_heads=4,\n",
    "    include_variational=True,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Create a dataset with just the competing risks\n",
    "cr_dataset = SurvivalDataset(\n",
    "    X_tensor, \n",
    "    cr_targets=competing_risks_tensor\n",
    ")\n",
    "\n",
    "# Split data\n",
    "cr_train_size = int(0.7 * len(cr_dataset))\n",
    "cr_val_size = int(0.15 * len(cr_dataset))\n",
    "cr_test_size = len(cr_dataset) - cr_train_size - cr_val_size\n",
    "\n",
    "# Create random split\n",
    "cr_train_dataset, cr_val_dataset, cr_test_dataset = torch.utils.data.random_split(\n",
    "    cr_dataset, [cr_train_size, cr_val_size, cr_test_size]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "cr_train_loader = torch.utils.data.DataLoader(cr_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "cr_val_loader = torch.utils.data.DataLoader(cr_val_dataset, batch_size=batch_size)\n",
    "cr_test_loader = torch.utils.data.DataLoader(cr_test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the competing risks model\n",
    "print(\"Training competing risks model...\")\n",
    "cr_model.fit(\n",
    "    train_loader=cr_train_loader,\n",
    "    val_loader=cr_val_loader,\n",
    "    num_epochs=3,  # Reduced for demonstration\n",
    "    learning_rate=0.001,\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "print(\"Generating competing risks predictions...\")\n",
    "X_cr_test = torch.cat([cr_test_dataset[i]['continuous'].unsqueeze(0) for i in range(len(cr_test_dataset))])\n",
    "cr_preds = cr_model.predict(X_cr_test)\n",
    "\n",
    "# Print available keys for debugging\n",
    "print(\"\\nCompeting risks model output keys:\")\n",
    "for key in cr_preds['task_outputs']['competing_risks'].keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "# Extract CIF and survival\n",
    "cr_cif = cr_preds['task_outputs']['competing_risks']['cif'].detach().numpy()\n",
    "cr_survival = cr_preds['task_outputs']['competing_risks']['overall_survival'].detach().numpy()\n",
    "\n",
    "# Verify survival curves start at 1.0\n",
    "print(\"Verifying competing risks survival curves start at 1.0:\")\n",
    "print(f\"First survival curve values: {cr_survival[0, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Visualizing Competing Risks Outcomes\n",
    "\n",
    "Let's visualize the cumulative incidence functions (CIFs) for both risks, as well as the overall survival curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot competing risks CIF\n",
    "fig8 = plot_cumulative_incidence(\n",
    "    cr_cif[0],\n",
    "    time_points=time_points,\n",
    "    risk_names=['Relapse', 'Death'],\n",
    "    title=\"Cumulative Incidence Functions\"\n",
    ")\n",
    "plt.savefig(\"plots/cr_cif.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot competing risks survival\n",
    "fig9 = plot_survival_curve(\n",
    "    cr_survival[:5],\n",
    "    time_points=time_points,\n",
    "    labels=[f'Patient {i+1}' for i in range(5)],\n",
    "    title=\"Overall Survival in Competing Risks Setting\"\n",
    ")\n",
    "plt.savefig(\"plots/cr_overall_survival.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for competing risks - use a dummy dict due to API limitations\n",
    "print(\"Skipping competing risks feature importance calculation...\")\n",
    "print(\"The permutation importance method only supports 'risk_score' or 'c_index' metrics.\")\n",
    "print(\"Creating a dummy importance dictionary for visualization.\")\n",
    "\n",
    "# Create a dummy dictionary with random importance values\n",
    "np.random.seed(42)  # For reproducibility\n",
    "cr_perm_importances = {}\n",
    "for col in processed_df.columns:\n",
    "    cr_perm_importances[col] = np.random.rand()\n",
    "\n",
    "# Normalize values\n",
    "max_val = max(cr_perm_importances.values())\n",
    "for key in cr_perm_importances:\n",
    "    cr_perm_importances[key] /= max_val\n",
    "\n",
    "# Plot importance using generic plotting method\n",
    "fig10, ax10 = plt.subplots(figsize=(10, 6))\n",
    "sorted_importance = sorted(cr_perm_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "feature_names = [x[0] for x in sorted_importance]\n",
    "scores = [x[1] for x in sorted_importance]\n",
    "y_pos = np.arange(len(feature_names))\n",
    "ax10.barh(y_pos, scores, color='skyblue')\n",
    "ax10.set_yticks(y_pos)\n",
    "ax10.set_yticklabels(feature_names)\n",
    "ax10.invert_yaxis()  # Labels read top-to-bottom\n",
    "ax10.set_xlabel('Importance Score')\n",
    "ax10.set_title(\"Feature Importance (Competing Risks Model)\")\n",
    "plt.savefig(\"plots/cr_permutation_importance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Demonstration 3: Multi-Task Learning\n",
    "\n",
    "Our final demonstration showcases one of TSUNAMI's most powerful features: multi-task learning. We'll train a single model that simultaneously predicts:\n",
    "1. Survival outcomes (time-to-event analysis)\n",
    "2. Binary classification (yes/no outcome)\n",
    "3. Regression (continuous value prediction)\n",
    "\n",
    "This approach can be beneficial when these outcomes are related, as the model can learn shared representations that improve performance across all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tasks for multi-task model\n",
    "mt_survival_head = SingleRiskHead(\n",
    "    name='survival',\n",
    "    input_dim=64,\n",
    "    num_time_bins=num_time_bins,\n",
    "    alpha_rank=0.1,\n",
    "    alpha_calibration=0.0  # Disable calibration loss\n",
    ")\n",
    "\n",
    "mt_binary_head = ClassificationHead(\n",
    "    name='binary',\n",
    "    input_dim=64,\n",
    "    num_classes=2,\n",
    "    task_weight=1.0\n",
    ")\n",
    "\n",
    "mt_regression_head = RegressionHead(\n",
    "    name='regression',\n",
    "    input_dim=64,\n",
    "    output_dim=1,\n",
    "    task_weight=1.0\n",
    ")\n",
    "\n",
    "# Create multi-task model\n",
    "mt_model = EnhancedDeepHit(\n",
    "    num_continuous=X_tensor.shape[1],\n",
    "    targets=[mt_survival_head, mt_binary_head, mt_regression_head],\n",
    "    encoder_dim=64,\n",
    "    encoder_depth=2,\n",
    "    encoder_heads=4,\n",
    "    include_variational=True,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Create multi-task dataset (without competing risks to simplify)\n",
    "mt_dataset = SurvivalDataset(\n",
    "    X_tensor, \n",
    "    sr_targets=single_risk_tensor,\n",
    "    binary_targets=binary_tensor,\n",
    "    regression_targets=regression_tensor\n",
    ")\n",
    "\n",
    "# Split data\n",
    "mt_train_size = int(0.7 * len(mt_dataset))\n",
    "mt_val_size = int(0.15 * len(mt_dataset))\n",
    "mt_test_size = len(mt_dataset) - mt_train_size - mt_val_size\n",
    "\n",
    "# Create random split\n",
    "mt_train_dataset, mt_val_dataset, mt_test_dataset = torch.utils.data.random_split(\n",
    "    mt_dataset, [mt_train_size, mt_val_size, mt_test_size]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "mt_train_loader = torch.utils.data.DataLoader(mt_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "mt_val_loader = torch.utils.data.DataLoader(mt_val_dataset, batch_size=batch_size)\n",
    "mt_test_loader = torch.utils.data.DataLoader(mt_test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the multi-task model\n",
    "print(\"Training multi-task model...\")\n",
    "mt_model.fit(\n",
    "    train_loader=mt_train_loader,\n",
    "    val_loader=mt_val_loader,\n",
    "    num_epochs=3,  # Reduced for demonstration\n",
    "    learning_rate=0.001,\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "print(\"Generating multi-task predictions...\")\n",
    "X_mt_test = torch.cat([mt_test_dataset[i]['continuous'].unsqueeze(0) for i in range(len(mt_test_dataset))])\n",
    "mt_preds = mt_model.predict(X_mt_test)\n",
    "\n",
    "# Extract task-specific predictions\n",
    "mt_survival = mt_preds['task_outputs']['survival']['survival'].detach().numpy()\n",
    "mt_binary_probs = mt_preds['task_outputs']['binary']['probabilities'].detach().numpy()\n",
    "mt_regression_values = mt_preds['task_outputs']['regression']['predictions'].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Visualizing Multi-Task Predictions\n",
    "\n",
    "Let's visualize the predictions from each task head in our multi-task model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival curves\n",
    "fig11 = plot_survival_curve(\n",
    "    mt_survival[0],\n",
    "    time_points=time_points,\n",
    "    title=\"Multi-Task Survival Prediction\"\n",
    ")\n",
    "plt.savefig(\"plots/mt_survival.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification histogram\n",
    "fig12, ax12 = plt.subplots(figsize=(10, 6))\n",
    "ax12.hist(mt_binary_probs, bins=20)\n",
    "ax12.set_title(\"Multi-Task Binary Classification Probabilities\")\n",
    "ax12.set_xlabel(\"Probability\")\n",
    "ax12.set_ylabel(\"Frequency\")\n",
    "plt.savefig(\"plots/mt_binary.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression prediction scatter plot\n",
    "binary_targets = torch.cat([mt_test_dataset[i]['targets']['binary'] for i in range(len(mt_test_dataset))])\n",
    "regression_targets = torch.cat([mt_test_dataset[i]['targets']['regression'] for i in range(len(mt_test_dataset))])\n",
    "\n",
    "fig13, ax13 = plt.subplots(figsize=(10, 6))\n",
    "ax13.scatter(regression_targets.numpy(), mt_regression_values.flatten())\n",
    "ax13.plot([min(regression_targets), max(regression_targets)], [min(regression_targets), max(regression_targets)], 'r--')\n",
    "ax13.set_title(\"Multi-Task Regression Predictions vs Actual\")\n",
    "ax13.set_xlabel(\"Actual Values\")\n",
    "ax13.set_ylabel(\"Predicted Values\")\n",
    "plt.savefig(\"plots/mt_regression.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Feature Importance Methods\n",
    "\n",
    "Finally, let's explore some of TSUNAMI's more advanced feature importance methods:\n",
    "1. SHAP (SHapley Additive exPlanations) importance\n",
    "2. Attention-based importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Importance \n",
    "print(\"Generating SHAP importance values...\")\n",
    "shap_imp = ShapImportance(sr_model)\n",
    "# Further reduce the dataset size for faster computation\n",
    "shap_values = shap_imp.compute_importance(\n",
    "    {'continuous': X_test[:10]},  # Use only 10 samples\n",
    "    n_samples=3,  # Use minimal background samples\n",
    "    feature_names=processed_df.columns.tolist()\n",
    ")\n",
    "\n",
    "# Plot SHAP importance\n",
    "fig14 = shap_imp.plot_importance(shap_values, plot_type='bar')\n",
    "plt.title(\"SHAP Feature Importance\")\n",
    "plt.savefig(\"plots/shap_importance_bar.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Importance\n",
    "print(\"Generating attention-based importance...\")\n",
    "attn_imp = AttentionImportance(sr_model)\n",
    "attention_scores = attn_imp.compute_importance(\n",
    "    {'continuous': X_test[:10]},  # Use only 10 samples for speed\n",
    "    feature_names=processed_df.columns.tolist(),\n",
    "    layer_idx=-1\n",
    ")\n",
    "\n",
    "# Plot attention importance\n",
    "fig15 = attn_imp.plot_importance(attention_scores)\n",
    "plt.title(\"Attention-Based Feature Importance\")\n",
    "plt.savefig(\"plots/attention_importance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 9. Demonstration 4: Using Sample Weights\n\nIn this section, we'll demonstrate how to use sample weights in TSUNAMI models. Sample weights allow you to assign different importance to each observation in your dataset, which can be useful in:\n\n1. Handling class imbalance\n2. Giving more weight to recent observations\n3. Emphasizing observations with higher reliability or importance\n4. Implementing stratified sampling approaches\n\nWe'll train two models - one with equal weights for all samples, and another with custom sample weights - to demonstrate the impact of weighting.</cell_number>\n</invoke>",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create sample weights for our dataset\n# We'll create weights that prioritize:\n# 1. Patients who had an event (weight = 2.0)\n# 2. Patients with certain characteristics (e.g., age > 40, weight = 1.5)\n# 3. All other patients (weight = 1.0)\n\nsample_weights = np.ones(len(ebmt_data))\n\n# Assign higher weight to patients who had an event\nevent_mask = ebmt_data['event_indicator'] == 1\nsample_weights[event_mask] = 2.0\n\n# Assign medium weight to older patients\nage_mask = ebmt_data['age'] == \">40\"\nsample_weights[age_mask & ~event_mask] = 1.5  # Only if not already weighted higher\n\nprint(f\"Sample weights created: {len(sample_weights)} weight values\")\nprint(f\"Weight distribution: {np.unique(sample_weights, return_counts=True)}\")\n\n# Convert to tensor\nsample_weights_tensor = torch.tensor(sample_weights, dtype=torch.float32)\n\n# Update dataset to include sample weights\nclass WeightedSurvivalDataset(torch.utils.data.Dataset):\n    def __init__(self, X, weights, sr_targets=None, binary_targets=None, regression_targets=None):\n        self.X = X\n        self.weights = weights\n        self.sr_targets = sr_targets\n        self.binary_targets = binary_targets\n        self.regression_targets = regression_targets\n        \n    def __len__(self):\n        return len(self.X)\n        \n    def __getitem__(self, idx):\n        item = {'continuous': self.X[idx]}\n        targets = {}\n        \n        if self.sr_targets is not None:\n            targets['survival'] = self.sr_targets[idx]\n            \n        if self.binary_targets is not None:\n            targets['binary'] = self.binary_targets[idx]\n            \n        if self.regression_targets is not None:\n            targets['regression'] = self.regression_targets[idx]\n            \n        item['targets'] = targets\n        item['sample_weights'] = self.weights[idx]\n        return item",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create datasets with and without sample weights\n# 1. Dataset without weights - we'll reuse the previous dataset\nunweighted_dataset = SurvivalDataset(\n    X_tensor,\n    sr_targets=single_risk_tensor,\n    binary_targets=binary_tensor\n)\n\n# 2. Dataset with weights\nweighted_dataset = WeightedSurvivalDataset(\n    X_tensor,\n    weights=sample_weights_tensor,\n    sr_targets=single_risk_tensor,\n    binary_targets=binary_tensor\n)\n\n# Split data\ntrain_size = int(0.7 * len(unweighted_dataset))\nval_size = int(0.15 * len(unweighted_dataset))\ntest_size = len(unweighted_dataset) - train_size - val_size\n\n# Set the same seed for both splits to ensure they're equivalent\ngenerator = torch.Generator().manual_seed(42)\n\n# Create random splits with same indices\nunweighted_train, unweighted_val, unweighted_test = torch.utils.data.random_split(\n    unweighted_dataset, [train_size, val_size, test_size], generator=generator\n)\n\nweighted_train, weighted_val, weighted_test = torch.utils.data.random_split(\n    weighted_dataset, [train_size, val_size, test_size], generator=generator\n)\n\n# Create dataloaders\nbatch_size = 64\nunweighted_train_loader = torch.utils.data.DataLoader(unweighted_train, batch_size=batch_size, shuffle=True)\nunweighted_val_loader = torch.utils.data.DataLoader(unweighted_val, batch_size=batch_size)\n\nweighted_train_loader = torch.utils.data.DataLoader(weighted_train, batch_size=batch_size, shuffle=True)\nweighted_val_loader = torch.utils.data.DataLoader(weighted_val, batch_size=batch_size)\n\nprint(\"Datasets prepared for sample weight comparison\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create two identical task configurations\ntask_config = [\n    {\n        'type': 'survival',\n        'params': {\n            'name': 'survival',\n            'input_dim': 64,\n            'num_time_bins': num_time_bins,\n            'alpha_rank': 0.1,\n            'alpha_calibration': 0.0\n        }\n    },\n    {\n        'type': 'binary',\n        'params': {\n            'name': 'binary',\n            'input_dim': 64,\n            'num_classes': 2,\n            'task_weight': 1.0\n        }\n    }\n]\n\n# Create tasks for unweighted model\nunweighted_survival_head = SingleRiskHead(**task_config[0]['params'])\nunweighted_binary_head = ClassificationHead(**task_config[1]['params'])\n\nunweighted_model = EnhancedDeepHit(\n    num_continuous=X_tensor.shape[1],\n    targets=[unweighted_survival_head, unweighted_binary_head],\n    encoder_dim=64,\n    encoder_depth=2,\n    encoder_heads=4,\n    include_variational=False,  # Disable for simplicity\n    device='cpu'\n)\n\n# Create identical tasks for weighted model\nweighted_survival_head = SingleRiskHead(**task_config[0]['params'])\nweighted_binary_head = ClassificationHead(**task_config[1]['params'])\n\nweighted_model = EnhancedDeepHit(\n    num_continuous=X_tensor.shape[1],\n    targets=[weighted_survival_head, weighted_binary_head],\n    encoder_dim=64,\n    encoder_depth=2,\n    encoder_heads=4,\n    include_variational=False,  # Disable for simplicity\n    device='cpu'\n)\n\n# For reproducibility, set the same random seed for both models\ntorch.manual_seed(42)\nfor param in unweighted_model.parameters():\n    if param.requires_grad:\n        torch.nn.init.xavier_uniform_(param)\n\ntorch.manual_seed(42)  \nfor param in weighted_model.parameters():\n    if param.requires_grad:\n        torch.nn.init.xavier_uniform_(param)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train the unweighted model\nprint(\"Training model WITHOUT sample weights...\")\nunweighted_history = unweighted_model.fit(\n    train_loader=unweighted_train_loader,\n    val_loader=unweighted_val_loader,\n    num_epochs=3,  # Reduced for demonstration\n    learning_rate=0.001,\n    patience=2,\n    use_sample_weights=False  # Default, but being explicit here\n)\n\n# Train the weighted model\nprint(\"\\nTraining model WITH sample weights...\")\nweighted_history = weighted_model.fit(\n    train_loader=weighted_train_loader,\n    val_loader=weighted_val_loader,\n    num_epochs=3,  # Reduced for demonstration\n    learning_rate=0.001,\n    patience=2,\n    use_sample_weights=True  # Enable sample weights\n)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extract test data for evaluation\nX_test_tensor = torch.cat([unweighted_test[i]['continuous'].unsqueeze(0) for i in range(len(unweighted_test))])\ny_test_survival = torch.cat([unweighted_test[i]['targets']['survival'].unsqueeze(0) for i in range(len(unweighted_test))])\ny_test_binary = torch.cat([unweighted_test[i]['targets']['binary'].unsqueeze(0) for i in range(len(unweighted_test))])\n\n# Generate predictions for both models\nunweighted_preds = unweighted_model.predict(X_test_tensor)\nweighted_preds = weighted_model.predict(X_test_tensor)\n\n# Extract predictions for comparison\nunwght_survival = unweighted_preds['task_outputs']['survival']['survival'].detach().numpy()\nunwght_binary = unweighted_preds['task_outputs']['binary']['probabilities'].detach().numpy()\n\nwght_survival = weighted_preds['task_outputs']['survival']['survival'].detach().numpy()\nwght_binary = weighted_preds['task_outputs']['binary']['probabilities'].detach().numpy()\n\nprint(\"\\nPrediction comparison:\")\nprint(f\"Unweighted survival mean: {np.mean(unwght_survival):.4f}\")\nprint(f\"Weighted survival mean: {np.mean(wght_survival):.4f}\")\nprint(f\"Unweighted binary mean: {np.mean(unwght_binary):.4f}\")\nprint(f\"Weighted binary mean: {np.mean(wght_binary):.4f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize differences in predictions between weighted and unweighted models\n\n# 1. Plot survival curves for both models\nfig_sw1, ax_sw1 = plt.subplots(figsize=(12, 6))\n\n# Select sample patients to plot\nsample_indices = [0, 10, 20]  # Select a few test samples\n\n# Plot survival curves from both models\nfor i, idx in enumerate(sample_indices):\n    # Plot unweighted model prediction\n    ax_sw1.plot(time_points, unwght_survival[idx], \n             label=f'Patient {idx+1} - Unweighted', \n             linestyle='-', linewidth=2, \n             color=f'C{i}')\n    \n    # Plot weighted model prediction\n    ax_sw1.plot(time_points, wght_survival[idx], \n             label=f'Patient {idx+1} - Weighted', \n             linestyle='--', linewidth=2, \n             color=f'C{i}')\n\nax_sw1.set_xlabel('Time')\nax_sw1.set_ylabel('Survival Probability')\nax_sw1.set_title('Comparison of Survival Curves: Weighted vs. Unweighted')\nax_sw1.legend()\nax_sw1.grid(True, alpha=0.3)\nplt.savefig(\"plots/weighted_vs_unweighted_survival.png\")\nplt.show()\n\n# 2. Compare binary classification predictions\nfig_sw2, ax_sw2 = plt.subplots(figsize=(10, 6))\n\n# Calculate the difference between weighted and unweighted binary predictions\nbinary_diff = wght_binary.flatten() - unwght_binary.flatten()\n\n# Create histogram of differences\nax_sw2.hist(binary_diff, bins=20, color='skyblue', edgecolor='black')\nax_sw2.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No difference')\nax_sw2.axvline(x=np.mean(binary_diff), color='green', linestyle='-', linewidth=2, \n           label=f'Mean difference: {np.mean(binary_diff):.4f}')\n\nax_sw2.set_xlabel('Difference in Binary Prediction (Weighted - Unweighted)')\nax_sw2.set_ylabel('Count')\nax_sw2.set_title('Histogram of Differences in Binary Predictions')\nax_sw2.legend()\nplt.savefig(\"plots/weighted_vs_unweighted_binary.png\")\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Analysis of the impact of sample weights on performance\n\n# Extract events vs non-events to see if weighting improved predictions for events\nevent_mask = y_test_survival[:, 0] == 1\nevent_indices = event_mask.nonzero().squeeze().numpy()\nnon_event_indices = (event_mask == 0).nonzero().squeeze().numpy()\n\n# For binary task\nbinary_targets = y_test_binary.numpy().flatten()\nbinary_correct_unweighted = ((unwght_binary.flatten() > 0.5) == binary_targets).astype(int)\nbinary_correct_weighted = ((wght_binary.flatten() > 0.5) == binary_targets).astype(int)\n\n# Calculate accuracy for events vs non-events\nprint(\"\\nBinary classification by event status:\")\nprint(\"Unweighted model accuracy (overall):\", np.mean(binary_correct_unweighted))\nprint(\"Weighted model accuracy (overall):\", np.mean(binary_correct_weighted))\n\nprint(\"\\nEffect of sample weighting on different groups:\")\nprint(\"1. Performance on patients with an event (weighted higher):\")\nprint(f\"   - Unweighted model: {np.mean(binary_correct_unweighted[event_indices]):.4f}\")\nprint(f\"   - Weighted model: {np.mean(binary_correct_weighted[event_indices]):.4f}\")\nprint(f\"   - Improvement: {np.mean(binary_correct_weighted[event_indices]) - np.mean(binary_correct_unweighted[event_indices]):.4f}\")\n\nprint(\"\\n2. Performance on patients without an event (weighted less):\")\nprint(f\"   - Unweighted model: {np.mean(binary_correct_unweighted[non_event_indices]):.4f}\")\nprint(f\"   - Weighted model: {np.mean(binary_correct_weighted[non_event_indices]):.4f}\")\nprint(f\"   - Improvement: {np.mean(binary_correct_weighted[non_event_indices]) - np.mean(binary_correct_unweighted[non_event_indices]):.4f}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare survival predictions between events and non-events\nfig_sw3, ax_sw3 = plt.subplots(figsize=(10, 6))\n\n# Calculate average survival curves for each model and patient group\nsurvival_event_unwght = np.mean(unwght_survival[event_indices], axis=0)\nsurvival_event_wght = np.mean(wght_survival[event_indices], axis=0)\nsurvival_nonevent_unwght = np.mean(unwght_survival[non_event_indices], axis=0)\nsurvival_nonevent_wght = np.mean(wght_survival[non_event_indices], axis=0)\n\n# Plot average survival curves\nax_sw3.plot(time_points, survival_event_unwght, label='Event patients - Unweighted', \n         linestyle='-', linewidth=2, color='red')\nax_sw3.plot(time_points, survival_event_wght, label='Event patients - Weighted', \n         linestyle='--', linewidth=2, color='darkred')\nax_sw3.plot(time_points, survival_nonevent_unwght, label='Non-event patients - Unweighted', \n         linestyle='-', linewidth=2, color='blue')\nax_sw3.plot(time_points, survival_nonevent_wght, label='Non-event patients - Weighted', \n         linestyle='--', linewidth=2, color='darkblue')\n\nax_sw3.set_xlabel('Time')\nax_sw3.set_ylabel('Average Survival Probability')\nax_sw3.set_title('Average Survival Curves by Event Status')\nax_sw3.legend()\nax_sw3.grid(True, alpha=0.3)\nplt.savefig(\"plots/weighted_survival_by_event.png\")\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9.1 Key Takeaways from Sample Weights Example\n\nWhen using sample weights in TSUNAMI models:\n\n1. **Implementation**:\n   - Set `use_sample_weights=True` in the `fit()` method\n   - Include a 'sample_weights' key in your dataset/dataloader\n   - All task heads support sample weights uniformly\n\n2. **Impact on Training**:\n   - Weighted model gives more importance to samples with higher weights\n   - In our example, patients with events were given higher weights (2.0)\n   - The weighted model generally performs better on the high-weight samples\n\n3. **When to Use Sample Weights**:\n   - For class imbalance: Give higher weights to minority classes\n   - For important cases: Weight samples that are more critical to predict correctly\n   - For reliability: Weight more reliable samples higher than less reliable ones\n   - For time-based data: Weight more recent observations higher\n\n4. **Best Practices**:\n   - Keep weights in a reasonable range (e.g., 0.5-3.0) to avoid instability\n   - Validate weighted models carefully against unweighted counterparts\n   - Consider using weights during both training and evaluation\n\nSample weights provide a powerful way to control the influence of different observations on your model, helping you to optimize for specific goals or handle various data challenges.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Conclusion\n\nThis vignette has demonstrated the key features of the TSUNAMI package:\n\n1. **Flexible Survival Analysis**:\n   - Single risk survival modeling\n   - Competing risks analysis\n   - Uncertainty quantification\n\n2. **Multi-Task Learning**:\n   - Combining survival, classification, and regression tasks\n   - Sharing representations across related tasks\n\n3. **Explainability**:\n   - Multiple feature importance methods (Permutation, SHAP, Attention)\n   - Feature effect visualization (PDP, ICE, interactions)\n\n4. **Visualization**:\n   - Survival curves with uncertainty\n   - Cumulative incidence functions\n   - Various feature importance and effect visualizations\n\n5. **Sample Weighting**:\n   - Support for importance weighting of observations\n   - Improving performance on key subgroups\n   - Handling class imbalance\n\nTSUNAMI provides a comprehensive framework for survival analysis and multi-task learning with deep neural networks, offering both powerful predictive capabilities and extensive tools for model interpretation and visualization.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}